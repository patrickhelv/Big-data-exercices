{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "499e46f5-12a4-46de-a212-90f2e936461e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading modules that we need\n",
    "import unittest\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from typing import Any\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection to Local spark Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Spark Master running in Docker\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyLocalNotebook2\") \\\n",
    "    .master(\"spark://localhost:7077\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify connection\n",
    "print(spark.version)\n",
    "print(\"Spark is running on\", spark.sparkContext.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f8c68e0-6bda-40a6-8588-381e51dc0a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_df\u001b[39m(table_name: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname of the table to load\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload(table_name)\n\u001b[1;32m----> 5\u001b[0m users_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/user/hive/warehouse/users\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m comments_df \u001b[38;5;241m=\u001b[39m load_df(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/user/hive/warehouse/comments\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m posts_df \u001b[38;5;241m=\u001b[39m load_df(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/user/hive/warehouse/posts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mload_df\u001b[1;34m(table_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_df\u001b[39m(table_name: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname of the table to load\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload(table_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \n",
    "def load_df(table_name: \"name of the table to load\") -> DataFrame:\n",
    "    return spark.read.format(\"delta\").load(table_name)\n",
    "\n",
    "users_df = load_df(\"users\")\n",
    "comments_df = load_df(\"comments\")\n",
    "posts_df = load_df(\"posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a2274f0-1bd8-4812-83d2-f793587e9548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Subtask 1: implenenting two helper functions\n",
    "Impelment these two functions:\n",
    "1. 'run_query' that gets a Spark SQL query and run it on df which is a Spark DataFrame; it returns the content of the first column of the first row of the DataFrame that is the output of the query;\n",
    "2. 'run_query2' that is similar to 'run_query' but instead of one DataFrame gets two; it returns the content of the first column of the first row of the DataFrame that is the output of the query.\n",
    "\n",
    "Note that the result of a Spark SQL query is itself a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5fce811-7dd2-4602-a243-18a36754c302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_query(query: \"a SQL query string\", df: \"the DataFrame that the query will be executed on\") -> Any:\n",
    "    df.createOrReplaceTempView(\"df\")\n",
    "    result = spark.sql(query)\n",
    "    r = result.collect()\n",
    "    return r[0][0]\n",
    "\n",
    "def run_query2(query: \"a SQL query string\", df1: \"DataFrame A\", df2: \"DataFrame B\") -> Any:\n",
    "    df1.createOrReplaceTempView(\"df1\")\n",
    "    df2.createOrReplaceTempView(\"df2\")\n",
    "    result = spark.sql(query)\n",
    "    r = result.collect()\n",
    "    return r[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7de10aba-7e77-4baa-af35-2b4e37916071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Subtask 2: writing a few queries\n",
    "Write the following queries in SQL to be executed by Spark in the next cell.\n",
    "\n",
    "1. 'q1': find the 'Id' of the most recently created post ('df' is 'posts_df') \n",
    "2. 'q2': find the number users\n",
    "3. 'q3': find the 'Id' of the user who posted most number of answers\n",
    "4. 'q4': find the number of questions\n",
    "5. 'q5': find the display name of the user who posted most number of comments\n",
    "\n",
    "Note that 'q1' is already available below as an example. Moreover, remmebr that Spark supports ANSI SQL 2003 so your queries have to comply with that standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3273ef7-7f4d-4b5d-bcf7-9935c952e26d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q1 = \"SELECT Id FROM df ORDER BY CreationDate DESC limit 1\"\n",
    "\n",
    "q2 = \"SELECT COUNT(*) as count FROM df\"\n",
    "\n",
    "q3 = \"SELECT OwnerUserId FROM df WHERE PostTypeId = 2 GROUP BY OwnerUserId ORDER BY COUNT(PostTypeId) DESC LIMIT 1\"\n",
    "\n",
    "q4 = \"SELECT COUNT(Id) as count FROM df WHERE PostTypeId = 1 ORDER BY count DESC LIMIT 1\"\n",
    "\n",
    "q5 = \"SELECT df1.DisplayName FROM df1 INNER JOIN df2 ON df1.Id = df2.UserId GROUP BY df1.DisplayName ORDER BY COUNT(df2.PostId) DESC LIMIT 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46400bac-c11c-4bee-81a8-67d42e3ca968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Subtask 3: validating the implementations by running the tests\n",
    "\n",
    "Run the cell below and make sure that all the tests run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ca3fb9-427c-425d-b334-0df9c208a21b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TestTask2(unittest.TestCase):\n",
    "    def test_q1(self):\n",
    "        # find the id of the most recent post\n",
    "        r = run_query(q1, posts_df)\n",
    "        self.assertEqual(r, 95045)\n",
    "\n",
    "    def test_q2(self):\n",
    "        # find the number of the users\n",
    "        r = run_query(q2, users_df)\n",
    "        self.assertEqual(r, 91616)\n",
    "\n",
    "    def test_q3(self):\n",
    "        # find the user id of the user who posted most number of answers\n",
    "        r = run_query(q3, posts_df)\n",
    "        self.assertEqual(r, 64377)\n",
    "\n",
    "    def test_q4(self):\n",
    "        # find the number of questions\n",
    "        r = run_query(q4, posts_df)\n",
    "        self.assertEqual(r, 28950)\n",
    "\n",
    "    def test_q5(self):\n",
    "        # find the display name of the user who posted most number of comments\n",
    "        r = run_query2(q5, users_df, comments_df)\n",
    "        self.assertEqual(r, \"Neil Slater\")\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe8a2e0c-7e72-410b-b385-00503c0bc136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Subtask 4: answering to questions about Spark related concepts\n",
    "\n",
    "Please answer the following questions. Write your answer in one to two short paragraphs. Don't copy-paste; instead, write your own understanding.\n",
    "\n",
    "1. What is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)'? \n",
    "2. When do you suggest using RDDs instead of using DataFrames?\n",
    "3. What is the main benefit of using DataSets instead of DataFrames?\n",
    "\n",
    "Write your answers in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c33ca21-1672-4c0c-a876-f73cbb8f65b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.\n",
    "RDD, are resilient distributed datasets, they are collections of objects that are capable of storing data across multiple nodes in a cluster and also allows them to perform processing tasks in parallel. It is fault tolerant when performing multiple transformation. RDD are mostly used for low level transformations because it does not impose a general schema and accessing data by column or attribute is not important. \n",
    "\n",
    "Dataframes are also a distributed collection of data points but the main difference compared to RDD is that the data is organized in columns. The data is organized in a schema to describe the data. Dataframes are also able to build relational query for the Spark Catalyst optimizer. \n",
    "\n",
    "Dataset contains both the benefits of Dataframes and rdds. Datasets are type safe, it is similar code syntax as RDDs but has also access to the Spark Catalyst optimizer. It is not available to use in Python. \n",
    "\n",
    "2.\n",
    "When we choose rdd over dataframes, we want either to use the map and actions functions. If we want to read unfiltered data, without a specific schema and the transformations a mostly low level we would use rdds.\n",
    "\n",
    "3. \n",
    "We use dataframes when we want structure in our data and when the transformations are high level. By structure we mean that we need to create a schema. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Task2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
