{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoapWNx6r5YZ"
      },
      "source": [
        "<h1><center>Cloud Computing  et informatique distribu√©e</center></h1>\n",
        "<h2>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "<center>Exercises: TD on Map Reduce and Spark</center>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNKFqyB6r5Yb"
      },
      "source": [
        "# First Part : Introduction to MapReduce - the origins in Hadoop\n",
        "\n",
        "MapReduce is a programming model useful in for big data processing and allows running a parallel and distributed  algorithm on a cluster.\n",
        "\n",
        "A MapReduce algorithm is composed of:\n",
        "<ul>\n",
        "    <li> a map function, which performs a first set of operations on the input data and, usually, produces a set of key-value pairs as output  </li>\n",
        "    <li> a reduce function, which performs an operation on the data coming as output from the map function grouped on the basis of the key.</li>\n",
        "</ul>\n",
        "\n",
        "The MapReduce algorithm can be run on a framework (i.e. Hadoop). In this case the framework will handle the process of\n",
        "running the algorithm on the distributed architecture running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance. The map function and the reduce function will then become independent procedures running in multiple instances in parallel.\n",
        "\n",
        "The map and the reduce procedures can be seen then as two main functions: map() et reduce().\n",
        "\n",
        "### Map\n",
        "\n",
        "In general the map() function is seen as a function that:\n",
        "<ul>\n",
        "    <li> takes as input one (or a series) key/value pair(s);  </li>\n",
        "    <li> processes each key/value pair according to the procedure it implements;</li>\n",
        "    <li> generates as output zero or more key/value pairs.</li>\n",
        "</ul>\n",
        "\n",
        "It is important to notice that:\n",
        "<ul>\n",
        "    <li> if the input data is composed by single values each sigle value can be seen as a key/value pair having an empty  value;  </li>\n",
        "    <li> the types of the input key/value pair and the output key/value pair(s) can be (and often are) different from each other;</li>\n",
        "    <li> output key/value pair can have a dummy key;</li>\n",
        "    <li> also the output key/value pair can have an empty value.</li>\n",
        "</ul>\n",
        "\n",
        "In the \"count words\" of the Spark introduction exercises we counting the number of occurrences of each word in the provided file. The map function would take as input a single line. This single line can be seen as a key/value pair having as key the whole line and as value an empty value. \n",
        "\n",
        "After this phase another map function will break the line into words and output a key/value pair for each word (after the phase that filters the stop-words and the non-letters). \n",
        "\n",
        "The following reduce step will consider as (key,value) pair a string representing the word and integer equal to 1 as value. \n",
        "\n",
        "\n",
        "### Shuffle\n",
        "Between the map() and reduce() functions, the data are shuffled in order to move together data sharing the same key.\n",
        "Data will be then processed by the reduce function. \n",
        "When you run a MapReduce procedure on a framework (i.e. Hadoop) using the available primitives the shuffle operation is fully and transparently handled by the framework. When you use Spark then you do not have to think explicitely about the shuffle operation when you run reduce-like operations over a key.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Reduce\n",
        "The reduce() function applies a procedure on all the pairs that share the same key.  \n",
        "\n",
        "It is important to notice that:\n",
        "<ul>\n",
        "    <li> the reduce function produces zero or more outputs for each group of key-value pairs  </li>\n",
        "    <li> the type of the output can be different from the input and can be also one or a set of key/value pair(s);</li>\n",
        "    <li> even in this case the key/value pair can have an empty value.</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "\n",
        "In the word count example, the reduce function takes the input values, sums them, and generates a single output containing a set of key/value pairs having as key the value of each word and as value the final sum.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "05bQ6mveDaUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0d00554-ef6a-4aa1-ce48-83c456d243ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "spark-3.2.4-bin-hadoop2.7/\n",
            "spark-3.2.4-bin-hadoop2.7/R/\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.2.4-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/workers.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/stop-workers.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/stop-worker.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/start-workers.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/start-worker.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/decommission-worker.sh\n",
            "spark-3.2.4-bin-hadoop2.7/sbin/decommission-slave.sh\n",
            "spark-3.2.4-bin-hadoop2.7/RELEASE\n",
            "spark-3.2.4-bin-hadoop2.7/jars/\n",
            "spark-3.2.4-bin-hadoop2.7/jars/netty-all-4.1.68.Final.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/metrics-jvm-4.2.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/metrics-json-4.2.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/metrics-jmx-4.2.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/metrics-graphite-4.2.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/metrics-core-4.2.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/macro-compat_2.12-1.1.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/lz4-java-1.7.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/logging-interceptor-3.12.12.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/libthrift-0.12.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/lapack-2.2.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-storageclass-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-scheduling-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-rbac-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-policy-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-node-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-networking-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-metrics-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-flowcontrol-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-extensions-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-events-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-discovery-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-core-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-coordination-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-common-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-certificates-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-batch-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-autoscaling-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-apps-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-apiextensions-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-model-admissionregistration-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kubernetes-client-5.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jul-to-slf4j-1.7.30.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jsr305-3.0.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/json4s-scalap_2.12-3.7.0-M11.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/json4s-jackson_2.12-3.7.0-M11.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/json4s-core_2.12-3.7.0-M11.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/json4s-ast_2.12-3.7.0-M11.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/json-1.8.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/joda-time-2.10.10.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jline-2.14.6.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jetty-sslengine-6.1.26.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jersey-server-2.34.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jersey-hk2-2.34.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jersey-container-servlet-core-2.34.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jersey-container-servlet-2.34.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jersey-common-2.34.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jersey-client-2.34.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.30.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jaxb-api-2.2.11.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/javassist-3.25.0-GA.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/janino-3.0.16.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jakarta.servlet-api-4.0.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jackson-module-scala_2.12-2.12.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jackson-datatype-jsr310-2.11.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.12.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jackson-databind-2.12.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jackson-core-2.12.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/jackson-annotations-2.12.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/ivy-2.5.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/httpcore-4.4.14.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/httpclient-4.5.13.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hk2-api-2.6.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-vector-code-gen-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-storage-api-2.7.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-shims-scheduler-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-shims-common-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-shims-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-shims-0.23-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-service-rpc-3.1.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-serde-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-metastore-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-llap-common-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-jdbc-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-exec-2.3.9-core.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-common-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-cli-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hive-beeline-2.3.9.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-hdfs-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-common-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-client-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-auth-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/hadoop-annotations-2.7.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/generex-1.0.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/flatbuffers-java-1.9.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/derby-10.14.2.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/curator-framework-2.7.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/curator-client-2.7.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-text-1.10.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-net-3.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-lang3-3.12.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-crypto-1.1.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-compress-1.21.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-compiler-3.0.16.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-codec-1.15.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/commons-beanutils-1.9.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/chill_2.12-0.10.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/chill-java-0.10.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/cats-kernel_2.12-2.1.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/breeze_2.12-1.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/breeze-macros_2.12-1.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/blas-2.2.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/avro-mapred-1.10.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/avro-ipc-1.10.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/avro-1.10.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/automaton-1.11-8.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/arrow-vector-2.0.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/arrow-memory-netty-2.0.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/arrow-memory-core-2.0.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/arrow-format-2.0.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/arpack-2.2.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/antlr4-runtime-4.8.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/annotations-17.0.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/algebra_2.12-2.0.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/aircompressor-0.21.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/RoaringBitmap-0.9.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/JTransforms-3.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/JLargeArrays-1.5.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/HikariCP-2.5.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/zstd-jni-1.5.0-4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/zookeeper-jute-3.6.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/zookeeper-3.6.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/xz-1.8.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/xml-apis-1.4.01.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/xercesImpl-2.12.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/xbean-asm9-shaded-4.20.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/velocity-1.5.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/univocity-parsers-2.9.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/transaction-api-1.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/tink-1.6.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/threeten-extra-1.5.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/stream-2.9.6.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spire_2.12-0.17.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spire-util_2.12-0.17.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spire-platform_2.12-0.17.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spire-macros_2.12-0.17.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-yarn_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-tags_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-tags_2.12-3.2.4-tests.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-streaming_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-sql_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-sketch_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-repl_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-network-shuffle_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-network-common_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-mllib_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-mllib-local_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-mesos_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-launcher_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-kvstore_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-kubernetes_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-hive_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-hive-thriftserver_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-graphx_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-core_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/spark-catalyst_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/snappy-java-1.1.8.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/snakeyaml-1.27.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/slf4j-log4j12-1.7.30.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/slf4j-api-1.7.30.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/shims-0.9.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/shapeless_2.12-2.3.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/scala-xml_2.12-1.2.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/scala-reflect-2.12.15.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/scala-library-2.12.15.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/scala-compiler-2.12.15.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/scala-collection-compat_2.12-2.1.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/rocksdbjni-6.20.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/pyrolite-4.30.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/py4j-0.10.9.5.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/parquet-jackson-1.12.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/parquet-hadoop-1.12.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/parquet-format-structures-1.12.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/parquet-encoding-1.12.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/parquet-common-1.12.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/parquet-column-1.12.2.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/paranamer-2.8.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/orc-shims-1.6.14.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/orc-mapreduce-1.6.14.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/orc-core-1.6.14.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/okio-1.14.0.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/okhttp-3.12.12.jar\n",
            "spark-3.2.4-bin-hadoop2.7/jars/objenesis-2.6.jar\n",
            "spark-3.2.4-bin-hadoop2.7/python/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pylintrc\n",
            "spark-3.2.4-bin-hadoop2.7/python/mypy.ini\n",
            "spark-3.2.4-bin-hadoop2.7/python/README.md\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_coverage/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_coverage/sitecustomize.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_coverage/coverage_daemon.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_coverage/conf/\n",
            "spark-3.2.4-bin-hadoop2.7/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.2.4-bin-hadoop2.7/python/setup.cfg\n",
            "spark-3.2.4-bin-hadoop2.7/python/run-tests\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/frame.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/__init__.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/series.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/__init__.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/utils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/types.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/typehints.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/serializers.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/map_ops.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/map_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/group_ops.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/group_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/functions.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/functions.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/avro/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/avro/functions.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/avro/functions.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/window.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/udf.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/udf.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/types.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/streaming.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/session.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/readwriter.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/group.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/functions.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/dataframe.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/context.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/conf.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/column.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/catalog.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/_typing.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/__init__.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_streaming.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_window.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_typehints.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_map.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_grouped_map.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_cogrouped_map.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resource/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resource/tests/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resource/tests/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resource/tests/test_resources.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resource/requests.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resource/requests.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resource/profile.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resource/profile.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resource/information.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resource/information.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resource/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/py.typed\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/frame.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/extensions.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/exceptions.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/datetimes.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/config.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/categorical.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/base.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/accessors.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/_typing.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/usage_logging/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/usage_logging/usage_logger.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/usage_logging/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/typedef/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/typedef/typehints.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/typedef/string_typehints.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/typedef/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/plot/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/plot/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/plot/test_series_plot_plotly.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/plot/test_series_plot_matplotlib.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/plot/test_series_plot.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/plot/test_frame_plot_plotly.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/plot/test_frame_plot_matplotlib.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/plot/test_frame_plot.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/indexes/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/indexes/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/indexes/test_datetime.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/indexes/test_category.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/indexes/test_base.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/testing_utils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/test_udt_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/test_string_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/test_num_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/test_null_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/test_datetime_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/test_date_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/test_complex_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/test_categorical_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/test_boolean_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/test_binary_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/data_type_ops/test_base.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_window.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_utils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_typedef.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_stats.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_sql.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_spark_functions.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_series_string.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_series_datetime.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_series_conversion.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_series.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_rolling.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_reshape.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_repr.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_rolling.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_expanding.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_ops_on_diff_frames.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_numpy_compat.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_namespace.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_internal.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_indexops_spark.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_indexing.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_groupby.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_frame_spark.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_extension.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_expanding.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_default_index.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_dataframe_spark_io.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_dataframe_conversion.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_dataframe.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_csv.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_config.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/tests/test_categorical.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/spark/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/spark/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/spark/utils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/spark/functions.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/spark/accessors.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/plot/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/plot/plotly.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/plot/matplotlib.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/plot/core.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/plot/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/missing/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/missing/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/missing/window.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/missing/series.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/missing/indexes.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/missing/groupby.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/missing/frame.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/missing/common.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/indexes/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/indexes/numeric.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/indexes/multi.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/indexes/datetimes.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/indexes/category.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/indexes/base.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/indexes/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/string_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/num_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/datetime_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/date_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/complex_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/categorical_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/boolean_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/binary_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/base.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/udt_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/null_ops.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/data_type_ops/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/window.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/utils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/strings.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/sql_processor.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/series.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/numpy_compat.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/namespace.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/mlflow.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/ml.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/internal.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/indexing.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/groupby.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/pandas/generic.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/util.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/tree.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/regression.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/recommendation.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/random.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/fpm.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/feature.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/evaluation.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/common.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/clustering.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/classification.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/_typing.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/tests/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/stat/test.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/install.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/files.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/context.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/conf.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/broadcast.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/accumulators.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/_typing.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/_globals.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/__init__.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_worker.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_util.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_serializers.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_rdd.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_profiler.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_join.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_install_spark.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_daemon.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_context.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_conf.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/testing/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/testing/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/testing/utils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/testing/streamingutils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/testing/sqlutils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/testing/pandasutils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/testing/mlutils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/testing/mllibutils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/tests/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/listener.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/kinesis.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/dstream.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/context.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/wrapper.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/util.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tuning.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tree.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tree.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/stat.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/regression.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/recommendation.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/pipeline.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/image.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/image.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/functions.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/functions.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/fpm.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/feature.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/evaluation.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/common.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/clustering.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/classification.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/base.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/_typing.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_util.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/param/shared.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/param/__init__.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/cloudpickle/\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/cloudpickle/cloudpickle_fast.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/cloudpickle/cloudpickle.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/cloudpickle/__init__.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/cloudpickle/compat.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/version.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/util.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/taskcontext.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/storagelevel.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/status.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/statcounter.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resultiterable.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/rdd.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/profiler.pyi\n",
            "spark-3.2.4-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/lib/\n",
            "spark-3.2.4-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-3.2.4-bin-hadoop2.7/python/lib/py4j-0.10.9.5-src.zip\n",
            "spark-3.2.4-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/index.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/conf.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/python_packaging.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/index.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/sql/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/sql/arrow_pandas.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/sql/index.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/pandas_on_spark/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/pandas_on_spark/types.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/pandas_on_spark/typehints.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/pandas_on_spark/transform_apply.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/pandas_on_spark/pandas_pyspark.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/pandas_on_spark/options.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/pandas_on_spark/index.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/pandas_on_spark/from_to_dbms.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/pandas_on_spark/faq.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/pandas_on_spark/best_practices.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/user_guide/arrow_pandas.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.streaming.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.ss.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.sql.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.ml.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/index.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.resource.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.pandas/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.pandas/window.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.pandas/series.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.pandas/indexing.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.pandas/index.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.pandas/groupby.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.pandas/general_functions.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.pandas/frame.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.pandas/ml.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.pandas/io.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.pandas/extensions.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/reference/pyspark.mllib.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/migration_guide/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_3.1_to_3.2.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.4_to_3.0.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.3_to_2.4.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.3.0_to_2.3.1_above.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.2_to_2.3.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_1.4_to_1.5.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_1.0_1.2_to_1.3.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/migration_guide/koalas_to_pyspark.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/migration_guide/index.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/getting_started/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/getting_started/quickstart_ps.ipynb\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/getting_started/quickstart_df.ipynb\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/getting_started/install.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/getting_started/index.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/development/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/development/testing.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/development/index.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/development/debugging.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/development/contributing.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/development/setting_ide.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/_templates/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/_templates/autosummary/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/_templates/autosummary/class_with_docs.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/_templates/autosummary/class.rst\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/_static/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/_static/copybutton.js\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/_static/css/\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/source/_static/css/pyspark.css\n",
            "spark-3.2.4-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-3.2.4-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-3.2.4-bin-hadoop2.7/python/.gitignore\n",
            "spark-3.2.4-bin-hadoop2.7/python/.coveragerc\n",
            "spark-3.2.4-bin-hadoop2.7/python/setup.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/run-tests.py\n",
            "spark-3.2.4-bin-hadoop2.7/python/run-tests-with-coverage\n",
            "spark-3.2.4-bin-hadoop2.7/bin/\n",
            "spark-3.2.4-bin-hadoop2.7/bin/sparkR\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-class\n",
            "spark-3.2.4-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/pyspark\n",
            "spark-3.2.4-bin-hadoop2.7/bin/docker-image-tool.sh\n",
            "spark-3.2.4-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-submit\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-sql2.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-sql.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-sql\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-shell\n",
            "spark-3.2.4-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/run-example\n",
            "spark-3.2.4-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-3.2.4-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/find-spark-home.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-3.2.4-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-3.2.4-bin-hadoop2.7/bin/beeline\n",
            "spark-3.2.4-bin-hadoop2.7/README.md\n",
            "spark-3.2.4-bin-hadoop2.7/conf/\n",
            "spark-3.2.4-bin-hadoop2.7/conf/workers.template\n",
            "spark-3.2.4-bin-hadoop2.7/conf/spark-env.sh.template\n",
            "spark-3.2.4-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
            "spark-3.2.4-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-3.2.4-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-3.2.4-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-3.2.4-bin-hadoop2.7/data/\n",
            "spark-3.2.4-bin-hadoop2.7/data/streaming/\n",
            "spark-3.2.4-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/iris_libsvm.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/multi-channel/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/license.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/kittens/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/images/license.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/als/\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-3.2.4-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/graphx/\n",
            "spark-3.2.4-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-3.2.4-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-3.2.4-bin-hadoop2.7/NOTICE\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-zstd.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-vis-timeline.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-respond.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-re2j.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-mustache.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-machinist.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-join.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-jodd.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-javassist.html\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-janino.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-datatables.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-blas.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-automaton.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-arpack.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-CC0.txt\n",
            "spark-3.2.4-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.2.4-bin-hadoop2.7/LICENSE\n",
            "spark-3.2.4-bin-hadoop2.7/examples/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredComplexSessionization.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/ExampleJdbcConnectionProvider.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/extensions/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/extensions/AgeExample.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/extensions/SparkSessionExtensionsTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithoutLoader.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithLoader.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/users.orc\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/people.csv\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/dir1/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/dir1/file3.json\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/dir1/file1.parquet\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/dir1/dir2/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/dir1/dir2/file2.parquet\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/META-INF/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/META-INF/services/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/META-INF/services/org.apache.spark.sql.jdbc.JdbcConnectionProvider\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/resources/META-INF/services/org.apache.spark.sql.SparkSessionExtensionsProvider\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/lm_with_elastic_net.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/fmRegressor.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/fmClassifier.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scripts/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/sql/arrow.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_sessionization.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/variance_threshold_selector_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/univariate_feature_selector_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/fm_regressor_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/fm_classifier_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredComplexSessionization.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.2.4-bin-hadoop2.7/examples/jars/\n",
            "spark-3.2.4-bin-hadoop2.7/examples/jars/spark-examples_2.12-3.2.4.jar\n",
            "spark-3.2.4-bin-hadoop2.7/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/tests/\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/tests/python_executable_check.py\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/tests/pyfiles.py\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/tests/py_container_checks.py\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/tests/decommissioning_cleanup.py\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/tests/decommissioning.py\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/tests/autoscale.py\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/dockerfiles/\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/decom.sh\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.2.4-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.2.4-bin-hadoop2.7/yarn/\n",
            "spark-3.2.4-bin-hadoop2.7/yarn/spark-3.2.4-yarn-shuffle.jar\n",
            "initialization successful!\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop2.7.tgz\n",
        "!tar zxvf spark-3.2.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local\")\n",
        "sc = SparkContext(conf = conf)\n",
        "print(\"initialization successful!\")\n",
        "\n",
        "import numpy as np\n",
        "import random as rn\n",
        "\n",
        "seed_value=0\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu8VlYm6DTv6"
      },
      "source": [
        "# MapReduce in Spark\n",
        "\n",
        "Remember that Spark engine providse a <code>map</code> operation on a RDD, a <code>reduce</code> operation on an RDD and a <code>reduceByKey</code> operation on a RDD.\n",
        "\n",
        "The map and the reduce operations in Spark allow to implement the MapReduce programming paradighm with some little differences to take into account while programming.\n",
        "\n",
        "### <code> Map</code>\n",
        "\n",
        "<li> the concept of the key is relaxed;</li>\n",
        "    <li>the <code>map</code> operations applies a function to all the elements of an RDD.</li>\n",
        "</li>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WCmTvYjBgL1"
      },
      "outputs": [],
      "source": [
        "from operator import add\n",
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "sorted(rdd.reduceByKey(add).collect())\n",
        "[('a', 2), ('b', 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_rhYZ3UDpOA"
      },
      "source": [
        "### <code> Reduce </code>\n",
        "\n",
        "\n",
        "<li> the concept of the key is relaxed;</li>\n",
        "    <li>the <code>reduce</code> operation applies a commutative and associative binary operator to all the elements of an RDD.</li>\n",
        "</li>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkcF6J1fETMH"
      },
      "outputs": [],
      "source": [
        "sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
        "15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si80OsuGEeb7"
      },
      "source": [
        "### <code> ReduceByKey </code>\n",
        "\n",
        "\n",
        "<li> the concept of the key is relaxed;</li>\n",
        "    <li>the <code>reduceByKey</code> operation applies an associative and commutative reduce function merging all the elements of an RDD using a key.</li>\n",
        "</li>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKCmiux9EudF"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "sorted(rdd.reduceByKey(add).collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9sL2wGOFFlv"
      },
      "source": [
        " # Some utility functions for the next exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvCMsVGAr5Yd"
      },
      "outputs": [],
      "source": [
        "def load_text(filename):\n",
        "    return sc.textFile(filename)\n",
        "\n",
        "mobydick = load_text('moby-dick.txt')\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "# Regular expression for removing all non-letter characters in the file.\n",
        "regex = re.compile('[^a-zA-Z ]')\n",
        "\n",
        "'''\n",
        "Removes any non-letter character from the given word.\n",
        "\n",
        "INPUT:\n",
        "        word: A word\n",
        "\n",
        "OUTPUT:\n",
        "        the input word without the non-letter characters.\n",
        "\n",
        "'''\n",
        "def remove_non_letters(word):\n",
        "    return regex.sub('', word)\n",
        "\n",
        "\n",
        "'''\n",
        "INPUT: \n",
        "        text: RDD where each element is a line of the input text file.\n",
        "        stopwords: Python list containing the stopwords.\n",
        "OUTPUT: \n",
        "        RDD where each element is a word from the input text file.\n",
        "'''\n",
        "def preprocess(text, stopwords):\n",
        "    words = text.flatMap(lambda line: line.split(\" \"))\\\n",
        "                .map(lambda word: remove_non_letters(word))\\\n",
        "                .filter(lambda word: len(word) > 0)\\\n",
        "                .map(lambda word: word.lower())\\\n",
        "                .filter(lambda word: word not in stopwords)\n",
        "    return words\n",
        "    \n",
        "'''\n",
        "INPUT: \n",
        "        stopwords_file: name of the file containing the stopwords.\n",
        "OUTPUT:\n",
        "        a Python list with the stopwords read from the file.\n",
        "'''\n",
        "def load_stopwords(stopwords_file):\n",
        "    stopwords = []\n",
        "    with open(stopwords_file) as file:\n",
        "        for sw in file:\n",
        "            stopwords.append(sw.strip())\n",
        "    return stopwords\n",
        "\n",
        "stopwords = load_stopwords(\"stopwords.txt\")\n",
        "words = preprocess(mobydick, stopwords)\n",
        "words.takeOrdered(10, key = lambda x: x)\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "#\n",
        "# ['aback',\n",
        "# 'aback',\n",
        "# 'abaft',\n",
        "# 'abaft',\n",
        "# 'abandon',\n",
        "# 'abandon',\n",
        "# 'abandon',\n",
        "# 'abandoned',\n",
        "# 'abandoned',\n",
        "# 'abandoned']\n",
        "#\n",
        "###################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj4vFazQr5Ye"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "INPUT:\n",
        "        words: RDD, where each element is word from the input text file (preprocessing already done!).\n",
        "OUTPUT:\n",
        "        RDD, where each element is (w, occ), w is a word and occ the number of occurrences of w.\n",
        "        The RDD is sorted by value in decreasing order.\n",
        "'''\n",
        "def word_count(words):    \n",
        "    occs = words.map(lambda word: (word, 1))\\\n",
        "                .reduceByKey(lambda x, y: x+y)\\\n",
        "                .sortBy(lambda f: f[1], ascending=False)\n",
        "    return occs\n",
        "    \n",
        "occs = word_count(words)\n",
        "occs.take(5)\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "#\n",
        "# [('whale', 891), ('one', 875), ('old', 436), ('man', 433), ('ahab', 417)]\n",
        "#\n",
        "###################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw-MRA6zr5Ye"
      },
      "source": [
        "# Example: Counting global number of integer elements using Spark\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We start generalizing the strict pattern of MapReduce and we want to start taking advance of the Spark functions.\n",
        "\n",
        "In this context, we take the mapReduce programming pattern as reference but we simplify the keys and we want to write the Spark code that counts the number of strings representing positive integers.\n",
        "\n",
        "We take as input  a text file containing also strings representing floats.\n",
        "    \n",
        "We can write a possible solution running acconding to the following steps:\n",
        "\n",
        "<ul>\n",
        "    <li> a map function filtering the strings and giving as output 1 for the integers elements  and 0 for the non integer elements. </li>\n",
        "    <li> a reduce function that performs the sum using the results coming from the map step.</li>\n",
        "</ul>\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGlucZtFr5Yf"
      },
      "source": [
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "As first we write the code of the function $int\\_map$ that performs the pre-process a text file splitting the strings when a space char is present.\n",
        "    \n",
        "The function has the following signature:\n",
        "<ul>\n",
        "<li> **Input.** A RDD $numbers$, where each element is a string from a text file.\n",
        "<li> **Output.** A RDD, where each element is $(i)$, is equal to a string\n",
        "</ul>\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<hr style=\"border:solid 2px;\">\n",
        "\n",
        "\n",
        "For example the content of the file can be:\n",
        "\n",
        "1 2 3.2 3 1.2 1.5\n",
        "\n",
        "4 5 1.1 6 7 5.4\n",
        "\n",
        "8 9 5.1 1 2 3 4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC704S7qr5Yf"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "creating the RDD from the text file\n",
        "\n",
        "'''\n",
        "rdd_from_file = sc.textFile(\"numbers.txt\")\n",
        "\n",
        "\n",
        "'''\n",
        "Parsing the lines of the RDD and splitting the strings\n",
        "\n",
        "INPUT:\n",
        "        an RDD\n",
        "\n",
        "OUTPUT:\n",
        "        an RDD of separate strings\n",
        "\n",
        "'''\n",
        "\n",
        "def int_map(numbers):\n",
        "    return numbers.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "rdd_split_numbers = int_map(rdd_from_file)\n",
        "\n",
        "print(rdd_split_numbers.take(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9rqnsHSr5Yf"
      },
      "source": [
        "Then we can proceed and write a possible solution for the integer count running acconding to the following steps:\n",
        "\n",
        "<ul>\n",
        "    <li> writing a map function filtering the single chars and giving as output 1 for the integers elements  and 0 for the non integer elements. Notice that in this step we are not including the key migrating to the Spark more flexible pattern.</li>\n",
        "    <li> a reduce function that is implemented by a sum that counts partial results coming from the map (using the dummy key) and returns the count (12 for this example).</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DlU4Wgcr5Yf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The function checking the value of the string and mapping each string to 1 or 0:\n",
        "1 if the string is the representation of an integer\n",
        "0 otherwise. \n",
        "\n",
        "Notice that we are simplifying the map-reduce pattern: the output is not composed by a (key-value) pair since keeping trace of the origin\n",
        "number value is not important for the final result.\n",
        "INPUT:\n",
        "        a string\n",
        "\n",
        "OUTPUT:\n",
        "        1 if the string is an integer (a string that does not contain any \"non numeric char\", 0 otherwise\n",
        "        Notice that:\n",
        "        \"-1\" is not a positive integer and the string contains \"- that is not a numeric value\"\n",
        "        \"1,1\" or \"1.1\" are not positive integer and the strings contain \",\" and \".\" respectively that are not numeric values\n",
        "\n",
        "'''\n",
        "\n",
        "def map_int_strings_to_1(numbers):\n",
        "    return numbers.map(lambda x : 1 if (x.isnumeric()) else 0)\n",
        "\n",
        "'''\n",
        "The function is performing a reduce just running a count that simply adds the list of numbers produced in the previous step\n",
        "INPUT:\n",
        "        an RDD having numbers as values\n",
        "\n",
        "OUTPUT:\n",
        "        the sum\n",
        "\n",
        "'''\n",
        "\n",
        "def reduce_sum(numbers_rdd):\n",
        "    return numbers_rdd.sum()\n",
        "\n",
        "\n",
        "print(reduce_sum(map_int_strings_to_1(rdd_split_numbers)))\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "#\n",
        "#  12 \n",
        "#\n",
        "###################################################\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFG7k2Byr5Yg"
      },
      "source": [
        "Of course it is not necessary to define different functions for the phases and you can write directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omu_eMP_r5Yg"
      },
      "outputs": [],
      "source": [
        "result = rdd_split_numbers.map(lambda x : 1 if (x.isnumeric()) else 0).sum()\n",
        "\n",
        "print(result)\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "#\n",
        "#  12 \n",
        "#\n",
        "###################################################\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-pYf5Jmr5Yg"
      },
      "source": [
        "In this first series of exercises we will learn how to think programs according to the MapReduce paradigm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XROtgpJpr5Yh"
      },
      "source": [
        "# 1 Numbers\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Suppose again to use the same file where lines are still made of integers. \n",
        "</font>\n",
        "</p>\n",
        "\n",
        "    \n",
        "## 1.1 Counting odd numbers and even numbers\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We must find how many even numbers and how many odd numbers are present the file following a MapReduce procedure. \n",
        "\n",
        "We would like to continue to generalize the procedure not including the keys but in this case we realize that maybe having a <code>map</code> function that produces a key-value pair can be useful for the reduce function that we apply after even in Spark context.\n",
        "    \n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We want to write the code of the function $even\\_odd\\_count$ that counts the number of occurrences \n",
        "The function has the following input and output:\n",
        "<ul>\n",
        "<li> **Input.** A RDD $numbers$, where each element is a number.\n",
        "<li> **Output.** A RDD, with two elements:  $('o', v)$, $v$ being the number of odd numbers and $('e', v)$ being the number of even numbers.\n",
        "</ul>\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdbF_rfer5Yj"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "creating the RDD from the integer text file\n",
        "\n",
        "'''\n",
        "rdd_split_numbers = int_map(sc.textFile(\"integer_numbers.txt\")) \n",
        "'''\n",
        "The function even_odd_count\n",
        "\n",
        "1) checks the value of an RDD of strings representing numbers and mapping each string to (\"e\", 1) or (\"o\", 1) and\n",
        "according to the number they represent. \n",
        "(\"e\", 1) if the string is the representation an even number\n",
        "(\"o\", 1) if the string is the representation an odd number\n",
        "\n",
        "2) reduces and counts the (key-value pairs )\n",
        "INPUT:\n",
        "        an RDD of string values\n",
        "\n",
        "OUTPUT:\n",
        "        an RDD [('o', c_o), ('e', c_e)]\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "'''############## WRITE YOUR CODE HERE ##############'''\n",
        "def even_odd_count(numbers_rdd):\n",
        "    numbers = \n",
        "    return numbers\n",
        "'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "\n",
        "print(even_odd_count(rdd_split_numbers).collect())\n",
        "\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "#\n",
        "#  [('o', 14), ('e', 5)]\n",
        "#\n",
        "###################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEc0q6EYr5Yj"
      },
      "source": [
        "## 1.2 Counting the occurrences of each number\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Suppose now that we want to count the number of occurrences of each number.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "We can go again step by step:\n",
        "\n",
        "\n",
        "\n",
        "<ol>\n",
        "<li> Define a function that produces pairs: </li>\n",
        "    [(1, 1), (2, 1), ...]\n",
        "\n",
        "<li>  think that data will be grouped together by a shuffle according to the key </li>\n",
        "    [[[1, 1]], [[2, 1]], [[3, 1], [3, 1]], [[4, 1], [4, 1]], ...]\n",
        "\n",
        "<li> apply a reduce that sums up the results </li>\n",
        "\n",
        "</ol>\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xamU0y2sr5Yj"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The function numbers_count\n",
        "\n",
        "1) checks the value of an RDD of strings representing numbers and mapping each string to (\"n\", 1)\n",
        "according to the number they represent. \n",
        "\n",
        "2) reduces and counts the (key-value pairs )\n",
        "INPUT:\n",
        "        an RDD of string values\n",
        "\n",
        "OUTPUT:\n",
        "        an RDD [('n_i', c_ni), ...)]\n",
        "\n",
        "'''\n",
        "\n",
        "'''############## WRITE YOUR CODE HERE ##############'''\n",
        "\n",
        "def numbers_count(numbers_rdd):\n",
        "    numbers= \n",
        "    return numbers\n",
        "\n",
        "'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "\n",
        "print(numbers_count(rdd_split_numbers).collect())\n",
        "\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "##  [('1', 2), ('1.5', 1), ('4', 1), ('5.4', 1), ('8', 1), ('9', 1), ('2', 2), ('3.2', 1), ('3', 2), ('1.2', 1), ('5', 1), ('1.1', 1), ('6', 1), ('7', 1), ('5.1', 1), ('4.1', 1)]\n",
        "###################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNsc5d4Ur5Yj"
      },
      "source": [
        "Notice that the map-reduce procedure applied for the word count and the number count is the same. Once you learn thinking in map-reduce philosophy you can easily re-use your patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAPnmbO6r5Yk"
      },
      "source": [
        "# 2 Bigrams\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "What is a Bigram? In our example a bigram is a couple of two consecutive words in a same line. \n",
        "    \n",
        "\n",
        "For example, the previous sentence contains the following bigrams: \"A bigram\", \"bigram is\", \"is a\", \"a couple\", etc.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "This exercise can be seen as a simple extension of the word count, take into account this while implementing your data structures and your code. \n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "As first step here is the to create a new RDD from the input text file './data/moby-dick.txt' that we already used in the first series of the exercises and that we saw again at the beginning of the lab. \n",
        "    \n",
        "We can reuse the code and recall the presence of the function (already defined) that allows to remove non-letter characters.\n",
        "</font>\n",
        "</p>\n",
        "<hr style=\"border:solid 2px;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohB7l1jvr5Yk"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "import re\n",
        "\n",
        "def load_text(filename):\n",
        "    return sc.textFile(filename)\n",
        "\n",
        "Removes any non-letter character from the given word.\n",
        "\n",
        "INPUT:\n",
        "        word: A word\n",
        "\n",
        "OUTPUT:\n",
        "        the input word without the non-letter characters.\n",
        "\n",
        "def remove_non_letters(word):\n",
        "    return regex.sub('', word)\n",
        "\n",
        "'''\n",
        "\n",
        "mobydick = load_text('moby-dick.txt')\n",
        "print(\"moby-dick text re-loaded in RDD and accessible in \\\"mobydick\\\" variable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qha5Tjtcr5Yk"
      },
      "source": [
        "## 2.1 - Bigrams\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Describe Map Reduce procedure that gives as output the number of different bigrams that appear all along the document (if the bigram \"is a\" appears twice in the document must be counted just one time).\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In the following a function that parses a line of the text and produces the set of bigrams fot the is provided. \n",
        "Give a look to this function, we will se it in detail later.</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXiIG6dfr5Yk"
      },
      "outputs": [],
      "source": [
        "from operator import add\n",
        "\n",
        "'''\n",
        "Returns the list of bigrams in a given text for each line (the end of a bigram search space is a line\n",
        "of text:\n",
        "\"This  sentence\n",
        "contains two different lines for looking for bigrams.\"\n",
        "INPUT:\n",
        "        a line of a text\n",
        "\n",
        "OUTPUT:\n",
        "        a list of bigrams for the text of the form b1_b2 where \"b1_b2\" is a string.\n",
        "\n",
        "'''\n",
        "\n",
        "def parse_bigrams(line):\n",
        "    bigrams = []\n",
        "    words = line.strip().split(\" \")\n",
        "    for i in range(len(words) - 1):\n",
        "        bigram_w1 = remove_non_letters(words[i].lower()) \n",
        "        bigram_w2 = remove_non_letters(words[i+1].lower())\n",
        "        if (len(bigram_w1) > 0) & (len(bigram_w2) > 0) :\n",
        "                bigrams.append(bigram_w1 + \"_\" + bigram_w2)\n",
        "    return bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaQzCbD0r5Yk"
      },
      "source": [
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Propose a Spark implementation of your procedure that follows a map-reduce approach.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6EmEksWr5Yk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Write here a procedure that given an RDD containing strings representing words returns the number of distinct bigrams.\n",
        "\n",
        "INPUT:\n",
        "        an RDD containing strings representing words\n",
        "\n",
        "OUTPUT:\n",
        "        the number of distinct bigrams.\n",
        "\n",
        "'''\n",
        "\n",
        "'''############## WRITE YOUR CODE HERE ##############'''\n",
        "\n",
        "counts = \n",
        "output = counts.collect()\n",
        "    \n",
        "    \n",
        "'''############## END OF THE EXERCISE ##############'''\n",
        " \n",
        "\n",
        "\n",
        "print(counts.take(5))\n",
        "print(counts.count())\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "#\n",
        "# [('call_me', 3), ('me_ishmael', 1), ('some_years', 1), ('years_agonever', 1), ('agonever_mind', 1)]\n",
        "# 100513\n",
        "###################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CobauOi3r5Yl"
      },
      "source": [
        "## 2.1 - Bigrams\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Sometimes when you write functions you do not realize that you can use a map-reduce series of steps.\n",
        "Look at the provided function that parses bigrams and outputs a string representation of them. Re-think the functions using map, reduce, and filter operators,\n",
        "    in Spark.\n",
        "    </font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Propose a Spark implementation of the procedure  that follows a map-reduce approach, parses lines of text, and \n",
        "gives as output pairs of strings representing bigrams. For this step you just can to re-use the\n",
        " functions we already studied, for example:\n",
        "<ul>\n",
        "    <li> map() </li>\n",
        "    <li> flatMap() </li>\n",
        "    <li> filter() </li>\n",
        "</ul>\n",
        "\n",
        "Notice that you can be more flexible in the structure of the keys.\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "367kKt5vr5Yl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Returns a list of pairs of strings representing bigrams in a given text.\n",
        "The procedure returns bigrams for each line (the search space of the bigrams is a line of text):\n",
        "\"This  sentence\n",
        "contains two different lines and ('sentence','contains') is not in the output.\"\n",
        "INPUT:\n",
        "        an RDD containing a line of a text\n",
        "\n",
        "OUTPUT:\n",
        "        a list of pairs bigrams for the text of the form (b1, b2) where ('b1', 'b2')\" is a pair of strings.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "'''############## WRITE YOUR CODE HERE ##############'''\n",
        "\n",
        "def parse_smart_parse_bigrams(text_rdd):\n",
        "    bigrams = \n",
        "    return bigrams\n",
        "'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "\n",
        "\n",
        "bigrams=parse_smart_parse_bigrams(mobydick)\n",
        "result = bigrams.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "print(result.count())\n",
        "print(result.take(5))\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "#\n",
        "#  100513\n",
        "# [(('me', 'ishmael'), 1), (('preciselyhaving', 'little'), 1), (('little', 'or'), 6), (('no', 'money'), 1), (('money', 'in'), 3)]\n",
        "###################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giKsXrMer5Yl"
      },
      "source": [
        "## 2.2 - Bigrams search\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Look at the two procedures that parse bigrams and provide the output in two different format: is the type of the\n",
        "   key influencing the map-reduce procedure? </font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "From the performances point of view which data representation is better according to your opinion? Explain and motivate your answer\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNnuTfhEr5Yl"
      },
      "source": [
        "## 2.3 - Bigrams\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Now that we start being confident with bigrams and Map Reduce, we can start analysing the bigrams and write a procedure that gives as output the top 5 different bigrams that appear all along the document.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Propose a Spark implementation of your procedure that transfomrs the key-value pairs and uses the top operation\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HiRmM3lr5Yl"
      },
      "outputs": [],
      "source": [
        "'''############## WRITE YOUR CODE HERE ##############'''\n",
        "inverted_result = \n",
        "'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "print(inverted_result.top(5))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "#\n",
        "# [(1726, ('of', 'the')), (1078, ('in', 'the')), (674, ('to', 'the')), (404, ('from', 'the')), (348, ('and', 'the'))]\n",
        "###################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub125zI0r5Ym"
      },
      "source": [
        "## 2.3 - Unique bigrams\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Try to re-use some of the steps you implemented in the previous exercise and provide the functions that counts the bigrams that appear only once in the text (if the bigram \"is a\" appears twice in the text it must not be counted in the final result).\n",
        "</font>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30v4TBU2r5Ym"
      },
      "outputs": [],
      "source": [
        "'''############## WRITE YOUR CODE HERE ##############'''\n",
        "\n",
        "\n",
        "unique_bigrams = \n",
        "\n",
        "\n",
        "'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "\n",
        "print(unique_bigrams.take(5))\n",
        "\n",
        "################# EXPECTED OUTPUT #################\n",
        "#\n",
        "#[(1, ('me', 'ishmael')), (1, ('preciselyhaving', 'little')), (1, ('no', 'money')), (1, ('way', 'i')), (1, ('driving', 'off'))]###################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi0mjecdr5Ym"
      },
      "source": [
        "# Second Part\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "In this set of exercises you will continue using Spark for implementing algorithms that apply computations on tables and matrices starting to go beyond the strict application of map reduce programming paradigm (and Philosophy).\n",
        "    \n",
        "You will be required to implement the matrix multiplication.\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhWFRh2Dr5Ym"
      },
      "source": [
        "# 3. Matrix Representation\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "**Please read carefully this section that presents how matrices will be represented in this assignment.**\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "Our input matrices are stored \n",
        "in textual files\n",
        "As an example, the file matrix-a.txt_ looks like as follows:\n",
        "<p>\n",
        "0 1 2 4<br>\n",
        "1 2 3 10<br>\n",
        "2 12 15 150<br>\n",
        "</p>\n",
        "</font>\n",
        "</p>\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "Each line is a row in a matrix $A$. The first number of the line is the \n",
        "row identifier (starting from 0), the subsequent values (separated by a whitespace)\n",
        "are the elements in each column of the row. The matrix represented in this file is the \n",
        "following:\n",
        "<p>\n",
        "<center>\n",
        "  $A= \\begin{bmatrix}\n",
        "    1 & 2 & 4   \\\\\n",
        "    2 & 3 & 10  \\\\\n",
        "    12 & 15 & 150\n",
        "\\end{bmatrix}$\n",
        "</center>\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "We provide the implementation of  basic functions to load a matrix from file, visualize it\n",
        "and get attributes.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "## 3.1 Function $loadMatrix$\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "The function $loadMatrix()$ loads a matrix from a file.\n",
        "It takes in the name of the file and returns an RDD containing the matrix.\n",
        "\n",
        "Each element of an RDD matrix is a key-value pair, where the key is the coordinate (row identifier, column identifier) of an element, and the value is the element itself.\n",
        "For instance, the RDD corresponding to the matrix $A$ is the following:\n",
        "<p>\n",
        "$( (0, 0), 1 ), ( (0, 1), 2 ), ( (0, 2), 4 ), ( (1, 0), 2 ), ( (1, 1), 3 ), ( (1, 2), 10 ), ( (2, 0), 12 ), ( (2, 1), 15 ), ( (2, 2), 150 ) $\n",
        "</p>\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "## 3.2 Function $shape$\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "The function $shape()$ takes in an RDD matrix and returns the size of the matrix as a pair $(nbRows, nbCols)$, where $nbRows$ (resp., $nbCols$) denotes the number of rows (resp., columns) of the matrix.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "## 3.3 Function $collect$\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "The function $collect()$ takes in an RDD matrix and returns a representation of the matrix as a Python list $L$. Each element of $L$ is itself a list that corresponds to a row in the matrix.\n",
        "For instance, the output of the function $collect$ for the matrix $A$ is as follows:   \n",
        "\n",
        "<p>\n",
        "\n",
        "$[ [1, 2, 4], [2, 3, 10], [12, 15, 150] ]$\n",
        "\n",
        "</p>\n",
        "\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "## 3.4 Function $nice$\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "The function $nice()$ prints the matrix in a nice and readable way.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**Execute the following cell in order to initialize the definition of the functions**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWFLoo6gr5Ym"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Loads a matrix from a file.\n",
        "INPUT: \n",
        "     the name of the input file\n",
        "OUTPUT:\n",
        "     an RDD containing the matrix\n",
        "'''\n",
        "def loadMatrix(filename):\n",
        "    # Load the file into an RDD matrix\n",
        "    matrix = sc.textFile(filename)\n",
        "    # Splits each line. Each element is a list [nbRow, e1, e2, ..., ej]\n",
        "    matrix = matrix.map(lambda line : line.split(' '))\n",
        "    # Convert each element to a number (the first is an integer, the others are float)\n",
        "    matrix = matrix.map(lambda row: [int(row[0])] + [float(row[i]) for i in range(1, len(row))])\n",
        "    # Get an RDD where each element is a key-value pair ((row, col), element)\n",
        "    matrix = matrix.flatMap(lambda row: [((row[0], j-1), row[j]) for j in range(1, len(row))])\n",
        "    return matrix\n",
        "\n",
        "'''\n",
        "Returns the number of rows and colums of the matrix\n",
        "INPUT: \n",
        "    An RDD representing a matrix\n",
        "OUTPUT: \n",
        "    the size of the matrix as (nbRows, nbCols)\n",
        "'''\n",
        "def shape(matrix):\n",
        "    M = collect(matrix)\n",
        "    if len(M) == 0:\n",
        "        return (0, 0)\n",
        "    else:\n",
        "        return (len(M), len(M[0]))\n",
        "\n",
        "'''\n",
        "Returns a matrix represented as a list of lists.\n",
        "INPUT: \n",
        "    an RDD representing a matrix\n",
        "OUTPUT: \n",
        "    the matrix represented as a list of lists.\n",
        "'''\n",
        "def collect(matrix):\n",
        "    # Obtain an RDD, where the key is the row identifier and the value is (colId, element)\n",
        "    matrix = matrix.map(lambda x: (x[0][0], (x[0][1], x[1])))\n",
        "    # Groups all the values in a row.\n",
        "    matrix = matrix.groupByKey()\n",
        "    # Sorts the element by row identifier.\n",
        "    matrix = matrix.sortByKey()\n",
        "    # Sort the elements by column identifier.\n",
        "    matrix = matrix.map(lambda x: sorted(list(x[1])))\n",
        "    # Now obtain an RDD, where each element is a list containing the elements of a row.\n",
        "    matrix = matrix.map(lambda row: [x[1] for x in row])\n",
        "    # Finally, return the RDD as a Python list.\n",
        "    return matrix.collect()\n",
        "    \n",
        "'''\n",
        "Prints the matrix in a nice way.\n",
        "INPUT: \n",
        "    the name of the matrix (var) and the matrix in the form of an RDD.\n",
        "OUTPUT:\n",
        "    - no output- it simply prints (shows) the matrix representation of the input\n",
        "'''\n",
        "def nice(var, matrix):\n",
        "    # Obtain a representation of the matrix as a Python list.\n",
        "    M = collect(matrix)\n",
        "    # Print the name of the matrix\n",
        "    print(\"Matrix \", var)\n",
        "    # Print the matrix and format the output nicely\n",
        "    print('\\n'.join([''.join(['{:12.2f}'.format(item) for item in row]) \n",
        "      for row in M]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGQUdSlDr5Yn"
      },
      "source": [
        "# 4. Matrix Addition\n",
        "\n",
        "<p align=\"justify\">\n",
        "<font size=\"3\">\n",
        "The code below loads two matrices $A$ and $B$ from file and calls the function $sum()$ to compute $A+B$.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "The function $sum()$ takes in:\n",
        "<ul>\n",
        "<li> $A$: an RDD containing the first matrix.\n",
        "<li> $B$: an RDD containing the second matrix.\n",
        "</ul>\n",
        "The function $sum()$ returns an RDD containing the matrix obtained by summing $A$ and $B$.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**Complete the definition of the function $sum()$ and execute the code**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1iF42Lyr5Yn"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Computes the sum of two matrices.\n",
        "INPUT: \n",
        "    two RDDs containing the input matrices\n",
        "OUTPUT: \n",
        "    the RDD containing the sum of the two input matrices\n",
        "'''\n",
        "        \n",
        "def sum(A, B):\n",
        "    '''############## WRITE YOUR CODE HERE ##############'''\n",
        "    '''############### AND COMPLETE FOLLOWING THE INSTRUCTIONS ##############'''\n",
        "    \n",
        "    # Each element of the RDD A and B is ((r,c), e), where e is an element of the matrix and (r, c) is the \n",
        "    # coordinate of the element in terms of row and column.\n",
        "    \n",
        "    # 1. Put the two RDDs A and B together. Use the transformation union. Remember that a transformation \n",
        "    # always returns a new RDD with the result of the transformation.\n",
        "    C = \n",
        "    \n",
        "    #2. Transforms the RDD C into one where the values having the same key (i.e., same row and column) \n",
        "    # are summed together. Which transformation are you going to use on C?\n",
        "    C =\n",
        "    # We return the RDD containing the sum of the two input matrices\n",
        "    return C\n",
        "\n",
        "'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "\n",
        "# Load matrix A from file and print it.\n",
        "A = loadMatrix(\"matrix-a.txt\")\n",
        "nice(\"A\", A)\n",
        "\n",
        "# Load matrix B from file and print it.\n",
        "B = loadMatrix(\"matrix-b.txt\")\n",
        "nice(\"B\", B)\n",
        "\n",
        "# Compute A+B and print it\n",
        "C = sum(A, B)\n",
        "nice(\"C\", C)\n",
        "\n",
        "############################################################## \n",
        "#YOU SHOULD OBTAIN THE FOLLOWING MATRIX C AS RESULT\n",
        "# 5.00        4.00        6.00      324.00       23.00\n",
        "# 3.00        6.00       13.00      333.00      423.00\n",
        "# 35.00       49.00      162.00       12.00        0.00\n",
        "##############################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULmQawncr5Yn"
      },
      "source": [
        "# 5. Scalar Multiplication\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "The code below calls the function $scalarMultiply()$ to obtain the matrix $c\\times A$, where $c$ is a scalar value.    \n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "The function $scalarMultiply()$ takes in:\n",
        "<ul>\n",
        "<li> $c$: a scalar value.\n",
        "<li> $M$: an RDD containing a matrix.\n",
        "</ul>\n",
        "The function $scalarMultiply()$ returns an RDD containing the matrix obtained by multiplying $c$ with the input matrix.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**Complete the definition of the function $scalarMultiply()$ and execute the code**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlglKjTKr5Yn"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Computes the scalar multiplication.\n",
        "INPUT: \n",
        "    a scalar value c and an RDD matrix M\n",
        "OUTPUT:\n",
        "    the RDD containing the matrix resulting from the scalar multiplication c * M.\n",
        "'''\n",
        "def scalarMultiply(c, M):\n",
        "    '''############## WRITE YOUR CODE HERE ##############'''\n",
        "    '''############### AND COMPLETE FOLLOWING THE INSTRUCTIONS ##############'''    \n",
        "    # Apply a transformation on M, so each element of the matrix M is multiplied by c\n",
        "    # Which transformation are you going to use?\n",
        "    R = \n",
        "    \n",
        "    return R\n",
        "'''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "\n",
        "\n",
        "# Prints \n",
        "nice(\"A\", A)\n",
        "nice(\"2*A\", scalarMultiply(2, A))\n",
        "\n",
        "############################################################## \n",
        "# THE RESULT SHOULD BE \n",
        "#2.00        4.00        8.00\n",
        "#4.00        6.00       20.00\n",
        "#24.00       30.00      300.00\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w48JB37Rr5Yo"
      },
      "source": [
        "# 6. Matrix Multiplication\n",
        "\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "We want to implement a function $multiply()$ to obtain the matrix $A \\times B$.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "The function $multiply()$ takes in:\n",
        "<ul>\n",
        "<li> $A$: an RDD containing the first matrix.\n",
        "<li> $B$: an RDD containing the second matrix.\n",
        "</ul>\n",
        "The function $multiply$ returns an RDD containing the matrix obtained by multiplying the first and the second matrix.\n",
        "The multiplication can only be computed if the number of columns of $A$ equals the number of rows of $B$.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "Let $A$ be an $n \\times m$ matrix and $B$ an $m \\times p$ matrix.\n",
        "The matrix $C = A \\times B$ is a $n \\times p$ matrix, where each element $c_{i, k}$ is computed as \n",
        "follows:\n",
        "<center>\n",
        "  $c_{i, k} = \\sum\\limits_{j=0}^{m-1} a_{i, j} \\cdot b_{j, k} \\quad\\quad (1)$ \n",
        "</center>\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "One possible implementation of this function is based on a MapReduce schema.\n",
        "Remember that in a MapReduce schema, the idea is to group elements by a key and apply a function to the elements \n",
        "that share the same key.\n",
        "As you can see, for a given $(i, k)$, the element of $A$ that is in the j-th column is multiplied by the value of $B$ that is in the j-th row, for any column $j$.\n",
        "Therefore, we can change the representation of $A$ and $B$ so that their elements are indexed by using $j$ as the key.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "More specifically, we can represent the matrix $A$ as follows:\n",
        "<center>    \n",
        "    $(j, (0, i, a_{i, j})) \\quad 0 \\leq i \\leq n-1 \\quad 0 \\leq j \\leq m-1  \\quad\\quad (2)$\n",
        "</center>    \n",
        "where the value $0$ in the triple $(0, i, a_{i, j})$ means that the element $a_{i, j}$ comes from the matrix $A$.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "Similarly, we can represent the matrix $B$ as follows:\n",
        "<center>    \n",
        "        $(j, (1, k, b_{j, k})) \\quad 0 \\leq j \\leq m-1 \\quad 0 \\leq k \\leq p-1 \\quad\\quad (3) $\n",
        "\n",
        "</center>    \n",
        "where the value $1$ in the triple $(1, k, b_{j, k})$ means that the element $b_{j, k}$ comes from the matrix $B$.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "<font size=\"3\">\n",
        "As a first step, we want to code two functions $transformA()$ and $transformB()$ to obtain the two representations of $A$ and $B$ respectively, as described in Equation (2) and (3).\n",
        "The two representations returned by both functions **must be RDDs**.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**Complete the definition of the functions $transformA()$ and $transformB()$ and execute the code**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joIjYXm3r5Yo"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "Transforms the RDD matrix A into an RDD as described in Equation (2)\n",
        "INPUT:\n",
        "    an RDD that contains data representing a matrix \n",
        "OUTPUT:\n",
        "    a matrix representation according to the Equation (2)\n",
        "'''\n",
        "\n",
        "def transformA(A):\n",
        "    '''############## WRITE YOUR CODE HERE ##############'''\n",
        "    # Transform A as indicated in the text above\n",
        "     A = \n",
        "    '''############## END OF THE FIRST PART OF THE EXERCISE ##############'''\n",
        "    return A\n",
        "\n",
        "\n",
        "'''\n",
        "Transforms the RDD matrix B into an RDD as described in Equation (3)\n",
        "INPUT:\n",
        "    an RDD that contains data representing a matrix \n",
        "OUTPUT:\n",
        "    a matrix representation according to the Equation (3)\n",
        "'''\n",
        "def transformB(B):\n",
        "    '''############## WRITE YOUR CODE HERE ##############'''\n",
        "    # Transform B as indicated in the text above\n",
        "    B = \n",
        "    '''############## END OF THE EXERCISE ##############'''\n",
        "    return B\n",
        "\n",
        "\n",
        "# Displayes the two matrices\n",
        "nice(\"A\", A)\n",
        "nice(\"B\", B)\n",
        "\n",
        "# Transforms them\n",
        "Atransformed = transformA(A)\n",
        "Btransformed = transformB(B)\n",
        "\n",
        "# Display the result.\n",
        "print(\"\\n********** Representation for A ************\\n\")\n",
        "print(Atransformed.collect())\n",
        "print(\"\\n********** Representation for B ************\\n\")\n",
        "print(Btransformed.collect())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2Ppe_Jmr5Yo"
      },
      "source": [
        "<p>\n",
        "<font size=\"3\">\n",
        "In order to group all the elements of both matrices by the key $j$, we need to merge the two RDDs $Atransformed$ and $Btransformed$.\n",
        "The function $merge()$ declared below takes in $Atransformed$ and $Btransformed$ and returns an RDD that results from the union of the two input RDDs.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**Complete the definition of the functions $merge()$ and execute the code**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGfX3Cc6r5Yo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Returns the union of Atransformed and Btransformed.\n",
        "INPUT:\n",
        "    two matrix representation following the Equation (2) and Equation (3) representation\n",
        "OUTPUT:\n",
        "    the union of Atransformed and Btransformed.\n",
        "'''\n",
        "def merge(Atransformed, Btransformed):\n",
        "    '''############## WRITE YOUR CODE HERE ##############'''\n",
        "    # Put together the two input RDDs\n",
        "    R = \n",
        "    '''############## END OF THE EXERCISE ##############'''\n",
        "\n",
        "    return R\n",
        "\n",
        "nice(\"A\", A)\n",
        "nice(\"B\", B)\n",
        "    \n",
        "merged = merge(Atransformed, Btransformed)    \n",
        "\n",
        "print(\"\\n********** Representation for merged ************\\n\")\n",
        "print(merged.collect())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB0tBzILr5Yp"
      },
      "source": [
        "<p>\n",
        "<font size=\"3\">\n",
        "Now we can group the values of the RDD $merged$ obtained above by their key $j$. \n",
        "We define a function $group()$ that returns an RDD obtained by grouping the values of the input RDD by their key.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**Complete the definition of the functions $group()$ and execute the code**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31qTpLUOr5Yp"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Returns an RDD where the values of the input RDD are grouped by their key.\n",
        "INPUT:\n",
        "    An RDD\n",
        "OUTPUT:\n",
        "    The values of the RDD grouped by key\n",
        "'''\n",
        "def group(merged):\n",
        "    '''############## WRITE YOUR CODE HERE ##############'''\n",
        "    # Groups the element of the input RDD by key.\n",
        "    R = \n",
        "    '''############## END OF THE EXERCISE ##############'''\n",
        "    return R\n",
        "\n",
        "nice(\"A\", A)\n",
        "nice(\"B\", B)\n",
        "    \n",
        "grouped = group(merged)    \n",
        "\n",
        "print(\"\\n********** Representation for grouped ************\\n\")\n",
        "L = grouped.collect()\n",
        "print('[')\n",
        "for l in L:\n",
        "    print(\"(\",l[0], \",\", end='', sep='')\n",
        "    print(\"[\", end='')\n",
        "    for el in l[1]:\n",
        "        print(el, end=\"\")\n",
        "    print(\"],\")\n",
        "print(']')\n",
        "\n",
        "######################################################################\n",
        "# Note that in the output, each element is (j, L), where\n",
        "# L is a list that contains all the elements in the j-th column of A\n",
        "# and all the elements in j-th row of B\n",
        "######################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbANrJU9r5Yp"
      },
      "source": [
        "<p>\n",
        "<font size=\"3\">\n",
        "Each element of the RDD $grouped$ obtained above is a key-value pair, where the key is the index $j$ and the value is a list $L$ containing all the triples corresponding to the elements of matrix $A$ in the $j-$th column  and the elements of matrix $B$  in the $j-$row, as follows: \n",
        "<p>\n",
        "<center>\n",
        "$(0, i, a_{i, j})\\ 0 \\leq i \\leq n-1 \\quad (1, k, b_{j, k})\\ 0 \\leq k \\leq p-1$\n",
        "</center>\n",
        "</p>\n",
        "<p>\n",
        "Remember that all triples corresponding to matrix $A$ have 0 as their first value, while those corresponding to matrix $B$ have 1.\n",
        "</p>\n",
        "<p>\n",
        "From Equation (1), you can see that the product $a_{i, j} \\cdot b_{j, k}$ contributes to the value $c_{i, k}$, for $0 \\leq i \\leq n-1$ and $0 \\leq k \\leq p-1$. \n",
        "Therefore, given the list $L$, we associate each value $a_{i, j} \\cdot b_{j, k}$  to the pair $(i, k)$.\n",
        "</p>\n",
        "<p>\n",
        "In other words, we now transform the RDD $grouped$ into an RDD where \n",
        "each element is a key-value pair, where the key is $(i, k)$ and the value is $a_{i, j} \\cdot b_{j, k}$.\n",
        "\n",
        "</p>    \n",
        "<p>\n",
        "In the code below, the function $multiplyElements()$ is given that takes in a value $(j, L)$ of the RDD $grouped$ \n",
        "and returns a list, where each element is a pair $((i, k), a_{i, j} \\cdot b_{j, k})$, $0 \\leq i \\leq n-1$ and $0 \\leq k \\leq p-1$.  \n",
        "</p>\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**Complete the definition of the function $groupProducts()$ that:\n",
        "    <ul>\n",
        "      <li> Takes in the RDD $grouped$.\n",
        "      <li> Applies the function $multiplyElements()$ to each element of $grouped$.\n",
        "      <li> Returns an RDD where each element is $((i, k), a_{i, j} \\cdot b_{j, k})$, $0 \\leq i \\leq n-1$ and $0 \\leq k \\leq p-1$. \n",
        "    </ul>\n",
        "    Execute the code.**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48PZgpTvr5Yp"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "'''\n",
        "Multiplies each element from matrix A with each element from \n",
        "matrix B in the list L (see description above).\n",
        "INPUT: \n",
        "    a value (j, L) of the RDD grouped.\n",
        "OUTPUT:\n",
        "    a list where each element is ((i, k), a_ij * b_jk)\n",
        "'''\n",
        "def multiplyElements(value):\n",
        "    j = value[0]\n",
        "    L = value[1]\n",
        "    \n",
        "    '''\n",
        "    The output key-value pairs.\n",
        "    '''\n",
        "    kv = []\n",
        "    '''\n",
        "    Maybe not necessary, we make sure that all triples with \n",
        "    the first element 0 (those from matrix A)\n",
        "    comes before any triple from matrix B.\n",
        "    '''\n",
        "    L = sorted(list(L))\n",
        "    \n",
        "    '''\n",
        "    For convenience, we store the triples from matrix A\n",
        "    and those from matrix B in two separate lists \n",
        "    LA and LB.\n",
        "    '''\n",
        "    sep = 0\n",
        "    while L[sep][0] == 0:\n",
        "        sep += 1\n",
        "    LA = [L[i] for i in range(0, sep)]\n",
        "    LB = [L[i] for i in range(sep, len(L))]\n",
        "    '''\n",
        "    For each element (0, i, a_ij) in LA  \n",
        "    and each element (1, k, b_jk) in LB, \n",
        "    we add the pair ((i, k), a_ij * b_jk) to kv.\n",
        "    '''\n",
        "    for a in LA:\n",
        "        for b in LB:\n",
        "            i = a[1]\n",
        "            k = b[1]\n",
        "            kv.append(((i, k), a[2]*b[2]))\n",
        "    return kv\n",
        "\n",
        "'''\n",
        "Returns an RDD where each value is a pair ((i, k), a_ij * b_jk)\n",
        "INPUT:\n",
        "    an RDD whose elements are grouped by key\n",
        "OUTPUT:\n",
        "     an RDD where each value is a pair ((i, k), a_ij * b_jk)\n",
        "'''\n",
        "def groupProducts(grouped):\n",
        "    '''############## WRITE YOUR CODE HERE ##############'''\n",
        "    # Apply a transformation to the input RDD to get an RDD as described in the text.\n",
        "    # Which tranformations are you going to use? map or flatMap?\n",
        "    R = \n",
        "    '''############## END OF THE EXERCISE ##############'''\n",
        "    return R\n",
        "\n",
        "\n",
        "\n",
        "nice(\"A\", A)\n",
        "nice(\"B\", B)\n",
        "\n",
        "print(\"\\n********** Representation for multipliedElements ************\\n\")\n",
        "multipliedElements = groupProducts(grouped)\n",
        "print(multipliedElements.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaiGxUtJr5Yp"
      },
      "source": [
        "<p>\n",
        "<font size=\"3\">\n",
        "Each element of the RDD $multipliedElements$ obtained above is $((i, k), a_{i, j} \\cdot b_{j, k})$, \n",
        "$0 \\leq i \\leq n-1$, $0 \\leq j \\leq m-1$, $0 \\leq k \\leq p-1$. \n",
        "From Equation (1), we can see that each $c_{i, k}$ is obtained by summing up the products $a_{i, j} \\cdot b_{j, k}$, $0 \\leq j \\leq m-1$.\n",
        "<p>\n",
        "Therefore, in order to obtain the matrix $C = A \\times B$, the only thing that we need to do is to sum all \n",
        "values $a_{i, j} \\cdot b_{j, k}$ associated with the same key $(i, k)$.\n",
        "</p>\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**Complete the definition of the function $getResult()$ that:\n",
        "    <ul>\n",
        "      <li> Takes in the RDD $multipliedElements$.\n",
        "      <li> Transforms the input RDD into one where each element is $((i, k), \\sum\\limits_{j=0}^{m-1} a_{i, j} \\cdot b_{j, k})$\n",
        "      <li> Returns the resulting RDD. \n",
        "    </ul>\n",
        "    Execute the code.We finally obtain the matrix $C$.**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU5oAzk-r5Yq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Returns an RDD where all the values with the same keys are summed.\n",
        "INPUT:\n",
        "    an RDD \n",
        "OUTPUT:\n",
        "     an RDD where all the values with the same keys are summed.\n",
        "'''\n",
        "def getResult(multipliedElements\n",
        "    '''############## WRITE YOUR CODE HERE ##############'''\n",
        "    # Apply a transformation to the input RDD so that all values with the same key are summed.\n",
        "    R = \n",
        "    '''############## END OF THE EXERCISE ##############'''\n",
        "    return R\n",
        "\n",
        "\n",
        "nice(\"A\", A)\n",
        "nice(\"B\", B)\n",
        "C = getResult(multipliedElements)\n",
        "nice(\"C\", C)\n",
        "\n",
        "############################################################## \n",
        "#YOU SHOULD OBTAIN THE FOLLOWING MATRIX C AS RESULT\n",
        "# 98.00      144.00       56.00     1038.00      869.00\n",
        "# 241.00      353.00      133.00     1767.00     1315.00\n",
        "# 3513.00     5169.00     1869.00    10683.00     6621.00\n",
        "##############################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvye-tDAr5Yq"
      },
      "source": [
        "<p>\n",
        "<font size=\"3\">\n",
        "We now wrap every function that we implemented above in only one function $multiply()$ that we can use any time we need to multiply two matrices.\n",
        "</font>\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "<hr style=\" border:none; height:2px;\">\n",
        " <font  size=\"3\" color='#91053d'>**Execute the code below to define the function $multiply()$**</font>\n",
        "<hr style=\" border:none; height:2px;\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFPweAqTr5Yq"
      },
      "outputs": [],
      "source": [
        "def multiply(A, B):\n",
        "  # lambda ((i, j), v): (j, (i, v))\n",
        "  left = A.map(lambda e: (e[0][1], (e[0][0], e[1])))\n",
        "  # lambda ((j, k), w): (j, (k, w))\n",
        "  right = B.map(lambda e: (e[0][0], (e[0][1], e[1])))\n",
        "  productEntries = left.join(right)\n",
        "  # lambda (x, ((i, v), (k, w))): ((i, k), (v * w))\n",
        "  productEntries = productEntries.map(lambda e: ( (e[1][0][0], e[1][1][0]), (e[1][0][1] * e[1][1][1]) ) )\\\n",
        "                  .reduceByKey(lambda x,y: x+y)\n",
        "  return productEntries"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "name": "BE4-Spark.ipynb"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}